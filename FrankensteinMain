import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score
from sklearn.preprocessing import MinMaxScaler, Imputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.utils.class_weight import compute_sample_weight
from joblib import dump

# Loading Dataset
df = pd.read_csv('dataset.csv')

# Preprocessing Steps
# Removing Duplicates
df = df.drop_duplicates().reset_index(drop=True)

# Checking and Fixing Null Value Errors
for col in df.columns:
    if df[col].isnull().values.any():
        print(f"Column {col} contains null values.")
        print(f"Dealing with null values using method:{'mean' if df[col].nunique()==np.nanmean(df[col]) else 'median'}")
        df[col] = df[col].fillna(df[col].mode()[0] if df[col].nunique()==np.nanmean(df[col]) else df[col].median())

# Drop irrelevant columns based on correlation matrix analysis
corr = df.corr()
to_drop = []
for i in range(len(corr.columns)):
    for j in range(i+1, len(corr.columns)):
        if abs(corr.iloc[i,j]) > 0.99:
            print(f"High Correlation between '{corr.columns[i]}' and '{corr.columns[j]}'. Dropping column '{corr.columns[j]}'.")
            to_drop.append(j)
df = df.drop(df.columns[to_drop], axis=1).reset_index(drop=True)

# Concatenating Categorical and Continuous variables
X = df.select_dtypes(['number', 'float64'])
y = df['Label'].astype('int').values

# Handling Highly Linear Relationship Between Two Independent Variables
from scipy.stats import pearsonr
cols = list(X.columns)
for i in range(len(cols)-1):
    for j in range(i+1, len(cols)):
        r, _, _ = pearsonr(X[cols[i]], X[cols[j]])
        if r >= .99:
            print(f"High Correlation ({r}) detected between '{cols[i]}' and '{cols[j]}'.")
            if cols[j] + '_pca' not in X.columns:
                pca = PCA(n_components=1)
                X[cols[j]] = pca.fit_transform(X[[cols[i], cols[j]]]).reshape(-1, 1)
                del X[cols[j]]
                X.insert(loc=cols.index(cols[j]), column=cols[j]+'_pca', value=X[cols[j]][:, 0])
            else:
                continue

# Normalization of input data
min_max_scaler = MinMaxScaler()
X = min_max_scaler.fit_transform(X)

# Encoding Categorical Variables Using One-Hot Encoding Technique
onehot = ColumnTransformer([('encoder', OneHotEncoder(), df.select_dtypes('category'))], remainder='passthrough')
X = onehot.fit_transform(X)

# Splitting Data into Train/Val/Test Sets
skf = StratifiedKFold(n_splits=3, shuffle=False, random_state=7)
for fold_, (indices_train, indices_val) in enumerate(skf.split(X, y)):
    # Extract Subsets Based On Indices Generated By K-fold Cross-validation
    X_fold = X[indices_fold_]
    y_fold = y

# Creating a dictionary for storing metrics across all folds
metrics = {}
for i, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    x_train, x_test, y_train, y_test = X[train_idx], X[val_idx], y[train_idx], y[val_idx]
    
    # Implementing Grid Search CV to find optimal hyperparameters
    param_grid = {'C': [0.001, 0.01, 0.1, 1., 10., 100.],
                   'penalty': ['l2'],
                   'solvers': ['newton-cg']}
    clf = LogisticRegression()
    grid = GridSearchCV(clf, param_grid, cv=5)
    grid.fit(x_train, y_train)
    
    # Scoring Model Performance on Validation Set
    y_pred = grid.predict(x_test)
    acc = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')
    roc_auc = roc_auc_score(y_test, grid.decision_function(x_test))
    
    # Applying Class Weightage to Address Imbalanced Data Issue
    class_weights = dict((cls, compute_sample_weight(class_weight='balanced', num_samples=len(x_train), classes=np.unique(y_train))) for cls in np.unique(y_train))
    clf.fit(x_train, y_train, sample_weight=list(class_weights.values()))
    
    # Recording Metrics For Each Fold In A Dictionary
    metrics[str(i)] = {"Accuracy": round(acc * 100, 2), "Precision": round(precision * 100, 2), "Recall": round(recall * 100, 2), "F1 Score": round(f1 * 100, 2), "ROC AUC": round(roc_auc * 100, 2)}
    
    # Save The Best Hyperparameters And Accuracy Score Obtained After Gridsearchcv
    best_params = grid.best_params_
    best_estimator = grid.best_estimator_
    filename = f"logreg_best_{i}.joblib"
    dump(best_estimator, filename)

# Printing Results Across All Folds
print("Results:\n", metrics)

# Calculating Mean Of All Folds
total = sum(list(map(lambda d: float(d["Accuracy"]), metrics.values())) + list(map(lambda d: float(d["Precision"]), metrics.values())) + list(map(lambda d: float(d["Recall"]), metrics.values())) + list(map(lambda d: float(d["F1 Score"]), metrics.values())) + list(map(lambda d: float(d["ROC AUC"]), metrics.values()))) / 3
average_accuracy = total // len(metrics)*3
average_precision = total // len(metrics)*3
average_recall = total // len(metrics)*3
average_f1_score = total // len(metrics)*3
average_roc_auc = total // len(metrics)*3
print("\nAverage Across All Folds:\n")
print(f"Accuracy:{round(average_accuracy*100, 2)}%\n")
print(f"Precision:{round(average_precision*100, 2)}%\n")
print(f"Recall:{round(average_recall*100, 2)}%\n")
print(f"F1 Score:{round(average_f1_score*100, 2)}%\n")
print(f"ROC AUC:{round(average_roc_auc*100, 2)}%\n")

# Plotting ROC Curve Using Decision Function Values Generated By Our Trained Model
fig, ax = plt.subplots()
probs = best_estimator.decision_function(X_val)
thresholds = np.linspace(-max(probs), max(probs), num=100)
tn, fp, fn, tp = confusion_matrix(Y_val, (probs > thresholds).astype(int)).ravel()
sensitivity = tp/(tp+fn)
specificity = tn/(tn+fp)
ax.plot([0, 1], [specificity, sensitivity], color='blue', linestyle="--")
for threshold in thresholds:
    clf.classes_[clf.predict(X_val).mean() <= threshold]
    TP = np.sum((clf.predict(X_val) == Y_val) & (clf.classes_.argmin(clf.decision_function(X_val)) == clf.classes_.index(1)) & (clf.decision_function(X_val) >= threshold))
    FN = np.sum((clf.predict(X_val) != Y_val) & (clf.classes_.argmin(clf.decision_function(X_val)) == clf.classes_.index(1)) & (clf.decision_function(X_val) >= threshold))
    TN = np.sum((clf.predict(X_val) == Y_val) & (clf.classes_.argmin(clf.decision_function(X_val)) != clf.classes_.index(1)) & (clf.decision_function(X_val) < threshold))
    FP = np.sum((clf.predict(X_val) != Y_val) & (clf.classes_.argmin(clf.decision_function(X_val)) != clf.classes_.index(1)) & (clf.decision_function(X_val) < threshold))
    ax.scatter(FP/TN if TN!=0 else 0, TP/FN if FN!=0 else 0, c=(threshold - min(probs))/(max(probs)-min(probs)), alpha=0.8)
    ax.set(xlabel='False Positive Rate (1 - Specificity)', ylabel='True Positive Rate (Sensitivity)', title='Logistic Regression ROC Curve')
    ax.legend(['ROC curve', f'Threshold={threshold:.2f}'])
    ax.text(0.05, 0.95, f'AUC: {roc_auc_score(Y_val, probs):.4f}', transform=ax.transAxes, fontsize='large')
    fig.savefig('roc_curve.png')

# Saving The Final Predictions From The Best Model To A CSV File
df = pd.DataFrame({'ID': df['Id'].reset_index().drop('id', axis=1), 'Target': best_estimator.predict(df)})
filename = 'model_output.csv'
df.to_csv(filename, index=False)
